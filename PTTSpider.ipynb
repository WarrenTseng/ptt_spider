{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Author: Chiu-Wang Tseng, 2017.8, BIME NTU\n",
    "# Web Spider for PTT\n",
    "\n",
    "import requests as rs\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml\n",
    "import time\n",
    "\n",
    "class PTTSpider(object):\n",
    "    def __init__(self, url):\n",
    "        #url = 'http://www.ptt.cc/bbs/BlackDesert/index.html'\n",
    "        \n",
    "        if url[:5] == 'https':\n",
    "            url = 'http' + url[5:]\n",
    "        self.url = url\n",
    "        print('Resolving the url...')\n",
    "        res = rs.get(url)\n",
    "        self.soup = bs(res.text, 'lxml')\n",
    "        print('Done')\n",
    "        \n",
    "    def _max_index(self, soup):\n",
    "        s = soup.select('a')\n",
    "        for a in s:\n",
    "            if a.text == '‹ 上頁':\n",
    "                url_len_diff = len('http://www.ptt.cc'+a.attrs['href']) - len(self.url)\n",
    "                max_index = int(a.attrs['href'][-5-url_len_diff:-5])+1\n",
    "                print('Max index:', max_index)\n",
    "        return max_index\n",
    "\n",
    "    def _extract_urls(self, max_index, target_index):\n",
    "        urls = list()\n",
    "        for i in range(max_index-target_index, max_index, 1):\n",
    "            _url = self.url[:-5]+str(i+1)+'.html'\n",
    "            urls.append(_url)\n",
    "        return urls\n",
    "    \n",
    "    def extract_content_pages(self, time_delay=1, proportion=1):\n",
    "        # time_delay: prevent the server aborting\n",
    "        max_index = self._max_index(self.soup)\n",
    "        if proportion>1 or proportion<0:\n",
    "            pass\n",
    "        else:\n",
    "            target_index = int(max_index*proportion)\n",
    "            print('Target index:', target_index)\n",
    "        urls = self._extract_urls(max_index, target_index)\n",
    "        pages = list()\n",
    "        print('Extracting all the pages from the urls...')\n",
    "        i = 0\n",
    "        try:\n",
    "            for _url in urls:\n",
    "                time.sleep(time_delay) # prevent aborting\n",
    "                i += 1\n",
    "                if i%10 == 0:\n",
    "                    print('url', i, '...')\n",
    "                res = rs.get(_url)\n",
    "                soup = bs(res.text, 'lxml')\n",
    "                s = soup.select('a')\n",
    "                for a in s:\n",
    "                    for k in a.attrs.keys():\n",
    "                        if k == 'href':\n",
    "                            if a.attrs['href'][0:len(url)-26] == self.url[-(len(self.url)-17):-10]+'M':\n",
    "                                url_ = 'http://www.ptt.cc'+a.attrs['href']\n",
    "                                pages.append(url_)\n",
    "            print('Done.')\n",
    "        except:\n",
    "            pass\n",
    "            print('Aborting by the server')\n",
    "        return pages\n",
    "    \n",
    "    def _extract_content(self, pages):\n",
    "        # pages: url array\n",
    "        htmls = list()\n",
    "        i = 0\n",
    "        print('Extracting all contents from the pages...')\n",
    "        for page in pages:\n",
    "            i += 1\n",
    "            if i%10 == 0:\n",
    "                print('page', i)\n",
    "            res = rs.get(page)\n",
    "            htmls.append(res)\n",
    "        print('Done')\n",
    "        return htmls\n",
    "    \n",
    "    def extract_soup(self, pages):\n",
    "        htmls = self._extract_content(pages)\n",
    "        soups = list()\n",
    "        for html in htmls:\n",
    "            soup = bs(html.text, 'lxml')\n",
    "            soups.append(soup)\n",
    "        return soups\n",
    "    \n",
    "    def resolve_soup(self, soups):\n",
    "        articles = list()\n",
    "        titles = list()\n",
    "        contents = list()\n",
    "        push_contents = list()\n",
    "        grades = list()\n",
    "        push_userids = list()\n",
    "        for soup in soups:\n",
    "            article = dict()\n",
    "            title = soup.select('title')\n",
    "            title = title[0].text\n",
    "            article['titles'] = title\n",
    "            titles.append(title)\n",
    "            meta = soup.select('meta')\n",
    "            for content in meta:\n",
    "                for key, val in content.attrs.items():\n",
    "                    if key == 'name' and val == 'description':\n",
    "                        article['contents'] = content.attrs['content']\n",
    "                        contents.append(content.attrs['content'])\n",
    "            push = dict()\n",
    "            push['tag'] = list()\n",
    "            push['userid'] = list()\n",
    "            push['content'] = list()\n",
    "            push_content = list()\n",
    "            push_userid = list()\n",
    "            span = soup.select('span')\n",
    "            grade = 0\n",
    "            for s in span:\n",
    "                for key, vals in s.attrs.items():\n",
    "                    for val in vals:\n",
    "                        if val == 'push-tag':\n",
    "                            push['tag'].append(s.text)\n",
    "                            if s.text == '推 ':\n",
    "                                grade += 1\n",
    "                            elif s.text == '→ ':\n",
    "                                pass\n",
    "                            elif s.text == '噓 ':\n",
    "                                grade -= 1\n",
    "                        elif val == 'push-userid':\n",
    "                            push['userid'].append(s.text)\n",
    "                            push_userid.append(s.text)\n",
    "                        elif val == 'push-content':\n",
    "                            push['content'].append(s.text[2:])\n",
    "                            push_content.append(s.text[2:])\n",
    "            push_userids.append(push_userid)\n",
    "            push['grade'] = grade\n",
    "            grades.append(grade)\n",
    "            article['pushes'] = push\n",
    "            push_contents.append(push_content)\n",
    "            articles.append(article)\n",
    "        return articles, [titles, contents, push_contents, push_userids, grades]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 選擇看板：car 板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving the url...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.ptt.cc/bbs/car/index.html' # car板的進板url\n",
    "ptt = PTTSpider(url) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將看板下指定比例的頁面內容抓出，並以bs4進行解析\n",
    "#### 其中 time_delay 參數是防止request太快被伺服器禁用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max index: 3694\n",
      "Target index: 3\n",
      "Extracting all the pages from the urls...\n",
      "Done.\n",
      "Extracting all contents from the pages...\n",
      "page 10\n",
      "page 20\n",
      "page 30\n",
      "page 40\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "pages = ptt.extract_content_pages(time_delay=1, proportion=0.001)\n",
    "soups = ptt.extract_soup(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顯示其中1篇文章的標題、內文及推文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[問題] Baleno行車紀錄器選擇 - 看板 car - 批踢踢實業坊'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles, attrs = ptt.resolve_soup(soups)\n",
    "index = 12\n",
    "articles[index]['titles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'看板上討論\\n有人提到裝響尾蛇A18會擋視線\\n不知道有其他板友建議嗎?\\n感謝\\n--\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[index]['contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我用HP F770小檔 還在接受範圍內',\n",
       " '不過我有把左右後照鏡貼廣角鏡片 以增加可視範圍就是',\n",
       " '強烈建議倒車顯影一定要裝，小巴死角很多',\n",
       " '不要裝後視鏡型的就好了~',\n",
       " '後照鏡型不好嗎，行車記錄器跟倒車影像整合',\n",
       " '後照鏡式會比原廠鏡大，前檔視野會影響']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[index]['pushes']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[index]['pushes']['grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Death44', 'Death44', 'rul543', 'scarface', 'mrporing', 'aweikids']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[index]['pushes']['userid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['推 ', '→ ', '推 ', '→ ', '推 ', '→ ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[index]['pushes']['tag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 至此，文章內容都已經被抓出來了，進一步進行分析、Text Mining的function還在建立中。\n",
    "## 在網頁中，有些內文的置入格式有出入，會導致內文抓不齊全"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 或是可以依個人需求，對其進行解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
