{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Author: Chiu-Wang Tseng, 2017.8, BIME NTU\n",
    "# Web Spider for PTT\n",
    "\n",
    "import requests as rs\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml\n",
    "import time\n",
    "\n",
    "class PTTSpider(object):\n",
    "    def __init__(self, url):\n",
    "        #url = 'http://www.ptt.cc/bbs/BlackDesert/index.html'\n",
    "        # main url \n",
    "        # normalize the url as http\n",
    "        if url[:5] == 'https':\n",
    "            url = 'http' + url[5:]\n",
    "        self.url = url\n",
    "        print('Resolving the url...')\n",
    "        # request the main url \n",
    "        res = rs.get(url)\n",
    "        # resolve the result by bs4\n",
    "        self.soup = bs(res.text, 'lxml')\n",
    "        print('Done')\n",
    "        \n",
    "    def _max_index(self, soup):\n",
    "        # return the max index of the pages under the main url\n",
    "        s = soup.select('a')\n",
    "        for a in s:\n",
    "            # find the max index by the href of '‹ 上頁' label\n",
    "            if a.text == '‹ 上頁':\n",
    "                url_len_diff = len('http://www.ptt.cc'+a.attrs['href']) - len(self.url)\n",
    "                max_index = int(a.attrs['href'][-5-url_len_diff:-5])+1\n",
    "                print('Max index:', max_index)\n",
    "        return max_index\n",
    "\n",
    "    def _extract_urls(self, max_index, target_index):\n",
    "        # extract all the url of the indexes\n",
    "        urls = list()\n",
    "        for i in range(max_index-target_index, max_index, 1):\n",
    "            _url = self.url[:-5]+str(i+1)+'.html'\n",
    "            urls.append(_url)\n",
    "        return urls\n",
    "    \n",
    "    def extract_content_pages(self, time_delay=1, proportion=1):\n",
    "        # extract all the content url from all the url of the indexes\n",
    "        # time_delay: prevent the server aborting\n",
    "        max_index = self._max_index(self.soup)\n",
    "        if proportion>1 or proportion<0:\n",
    "            pass\n",
    "        else:\n",
    "            target_index = int(max_index*proportion)\n",
    "            print('Target index:', target_index)\n",
    "        urls = self._extract_urls(max_index, target_index)\n",
    "        pages = list()\n",
    "        print('Extracting all the pages from the urls...')\n",
    "        i = 0\n",
    "        try:\n",
    "            for _url in urls:\n",
    "                time.sleep(time_delay) # prevent aborting\n",
    "                i += 1\n",
    "                if i%10 == 0:\n",
    "                    print('url', i, '...')\n",
    "                # resolving the url\n",
    "                res = rs.get(_url)\n",
    "                soup = bs(res.text, 'lxml')\n",
    "                s = soup.select('a')\n",
    "                for a in s:\n",
    "                    for k in a.attrs.keys():\n",
    "                        if k == 'href':\n",
    "                            if a.attrs['href'][0:len(url)-26] == self.url[-(len(self.url)-17):-10]+'M':\n",
    "                                url_ = 'http://www.ptt.cc'+a.attrs['href']\n",
    "                                pages.append(url_)\n",
    "            print('Done.')\n",
    "        except:\n",
    "            pass\n",
    "            print('Aborting by the server')\n",
    "        return pages\n",
    "    \n",
    "    def _extract_content(self, pages):\n",
    "        # extract all the contents by the content urls\n",
    "        # pages: url array\n",
    "        htmls = list()\n",
    "        i = 0\n",
    "        print('Extracting all contents from the pages...')\n",
    "        for page in pages:\n",
    "            i += 1\n",
    "            if i%10 == 0:\n",
    "                print('page', i)\n",
    "            res = rs.get(page)\n",
    "            htmls.append(res)\n",
    "        print('Done')\n",
    "        return htmls\n",
    "    \n",
    "    def extract_soup(self, pages):\n",
    "        # extract all the contents and resolve them by bs4\n",
    "        htmls = self._extract_content(pages)\n",
    "        soups = list()\n",
    "        for html in htmls:\n",
    "            soup = bs(html.text, 'lxml')\n",
    "            soups.append(soup)\n",
    "        return soups\n",
    "    \n",
    "    def show_titles(self, soups):\n",
    "        # extract the titles of the contents\n",
    "        titles = list()\n",
    "        for soup in soups:\n",
    "            title = soup.select('title')\n",
    "            title = title[0].text\n",
    "            titles.append(title)\n",
    "        return titles\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 選擇看板：car 板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving the url...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.ptt.cc/bbs/car/index.html' # car板的進板url\n",
    "ptt = PTTSpider(url) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將看板下指定比例的頁面內容抓出，並以bs4進行解析\n",
    "#### 其中 time_delay 參數是防止request太快被伺服器禁用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max index: 3678\n",
      "Target index: 3\n",
      "Extracting all the pages from the urls...\n",
      "Done.\n",
      "Extracting all contents from the pages...\n",
      "page 10\n",
      "page 20\n",
      "page 30\n",
      "page 40\n",
      "page 50\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "pages = ptt.extract_content_pages(time_delay=1, proportion=0.001)\n",
    "soups = ptt.extract_soup(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顯示其中10篇文章的title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Re: [討論] U6嚴重車禍 - 看板 car - 批踢踢實業坊',\n",
       " '[情報] 售價調降Subaru Levorg、WRX、WRX STi - 看板 car - 批踢踢實業坊',\n",
       " '[情報] Intel,Mobileye打造100輛LV4自駕車做路試 - 看板 car - 批踢踢實業坊',\n",
       " 'Re: [問題] 請問還有需要再去試86嗎(已訂220i) - 看板 car - 批踢踢實業坊',\n",
       " '[分享] 逼車教學技巧 - 看板 car - 批踢踢實業坊',\n",
       " '[情報] 到了2050年 自駕車商機產值達7兆美元/年 - 看板 car - 批踢踢實業坊',\n",
       " '[問題] 側裙撞傷求台南鈑金廠 - 看板 car - 批踢踢實業坊',\n",
       " '[問題] 150購車預算 東洋賓士vs德國寶馬? - 看板 car - 批踢踢實業坊',\n",
       " '[問題] 撞傷板金與乙式出險問題 - 看板 car - 批踢踢實業坊',\n",
       " '[情報] Daihatsu發表兩台概念車 比神A好看 (圖) - 看板 car - 批踢踢實業坊']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = ptt.show_titles(soups)\n",
    "titles[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 至此，文章內容都已經被抓出來了，進一步進行分析、Text Mining的function還在建立中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 或是可以依個人需求，對其進行解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
